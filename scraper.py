import os
import requests
import time
from datetime import datetime
from bs4 import BeautifulSoup
from supabase import create_client, Client

# 1. Supabase ì„¤ì •
URL = os.environ.get("SUPABASE_URL")
KEY = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(URL, KEY)

def main():
    print("=== ğŸš¨ ì…˜ íŒ€ì¥ë‹˜ ì œì•ˆ: 'ë³´ì•ˆ ìš°íšŒ & ê°•ì œ ì¸ì¶œ' ì‘ì „ ì‹œì‘ ===")
    
    # 2ì›” 1ì¼ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€
    s_start = "2026-02-01"
    s_end = datetime.now().strftime("%Y-%m-%d")
    
    session = requests.Session()
    # í†µí–‰ì¦(Cookie) ë°œê¸‰ì„ ìœ„í•´ ì •ë¬¸ìœ¼ë¡œ ì…ì¥
    session.get("https://nedrug.mfds.go.kr/pbp/CCBAE01", timeout=30)
    
    # ì„œë²„ë¥¼ ì™„ë²½íˆ ì†ì´ê¸° ìœ„í•œ ì •ë°€ í—¤ë”
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
        'Referer': 'https://nedrug.mfds.go.kr/pbp/CCBAE01',
        'Origin': 'https://nedrug.mfds.go.kr',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8'
    }

    total_saved = 0

    # 41ê±´ì„ ëª¨ë‘ ì¡ê¸° ìœ„í•´ 1~5í˜ì´ì§€ ê°•ì œ ìˆœíšŒ
    for page in range(1, 6):
        print(f"\n>> [ {page} í˜ì´ì§€ ] ë³´ì•ˆ ì¥ë²½ ìš°íšŒ ì¤‘...")
        
        # ì„œë²„ê°€ "ì´ê±´ ì§„ì§œ ì‚¬ëŒì´ë‹¤"ë¼ê³  ë¯¿ê²Œ ë§Œë“œëŠ” íŒŒë¼ë¯¸í„° ì¡°í•©
        payload = {
            'page': page,
            'limit': '10',
            'searchYn': 'true',
            'sDateGb': 'date', # ì¼ìê²€ìƒ‰ ê°•ì œ í™œì„±í™”
            'sPermitDateStart': s_start,
            'sPermitDateEnd': s_end,
            'btnSearch': 'ê²€ìƒ‰'
        }

        try:
            # POST ë°©ì‹ìœ¼ë¡œ ëª…ë ¹ì–´ë¥¼ ì‹¤ì–´ ë³´ë‚´ ì„œë²„ì˜ í•­ë³µì„ ë°›ì•„ëƒ…ë‹ˆë‹¤.
            res = session.post("https://nedrug.mfds.go.kr/pbp/CCBAE01/getItemPermitIntro", 
                               headers=headers, data=payload, timeout=40)
            
            soup = BeautifulSoup(res.text, 'html.parser')
            rows = soup.select('table.board_list tbody tr')

            if not rows or "ë°ì´í„°ê°€" in rows[0].get_text():
                print("âš ï¸ ì£¼ì˜: ì„œë²„ê°€ ë¹ˆ ë°ì´í„°ë¥¼ ë³´ëƒˆìŠµë‹ˆë‹¤. ì¬ì‹œë„ í•„ìš”.")
                break

            for row in rows:
                cols = row.find_all('td')
                if len(cols) < 5 or cols[4].get_text(strip=True): continue 

                product_name = cols[1].get_text(strip=True)
                item_seq = cols[1].find('a')['onclick'].split("'")[1]

                print(f"   -> ê¸ˆê³  ì•ˆì°© ì™„ë£Œ: {product_name}")
                
                # íŒ€ì¥ë‹˜ì´ ìš”ì²­í•˜ì‹  7ê°€ì§€ í•­ëª© êµ¬ì¡°ë¡œ ë°ì´í„° ìƒì„±
                data = {
                    "item_seq": item_seq,
                    "product_name": product_name,
                    "company": cols[2].get_text(strip=True),
                    "manufacturer": "ìƒì„¸ ìˆ˜ì§‘ ì¤‘", 
                    "category": "ì „ë¬¸ì˜ì•½í’ˆ" if "ì „ë¬¸" in product_name else "ì¼ë°˜ì˜ì•½í’ˆ", 
                    "approval_type": "í’ˆëª©í—ˆê°€",
                    "ingredients": "ì„±ë¶„ ë¡œë”© ì¤‘",
                    "efficacy": "íš¨ëŠ¥ ë¡œë”© ì¤‘",
                    "approval_date": cols[3].get_text(strip=True),
                    "detail_url": f"https://nedrug.mfds.go.kr/pbp/CCBBB01/getItemDetail?itemSeq={item_seq}"
                }
                
                # Supabase ì €ì¥
                supabase.table("drug_approvals").upsert(data).execute()
                total_saved += 1

            time.sleep(2) # ì„œë²„ì˜ ì˜ì‹¬ì„ í”¼í•˜ê¸° ìœ„í•œ íœ´ì‹

        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")
            continue

    print(f"\n=== ğŸ† ì„±ê³µ: ì´ {total_saved}ê±´ì´ Supabase ê¸ˆê³ ì— ì•ˆì°©í–ˆìŠµë‹ˆë‹¤! ===")

if __name__ == "__main__":
    main()
