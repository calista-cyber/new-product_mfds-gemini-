import os
import requests
import time
from datetime import datetime
from bs4 import BeautifulSoup
from supabase import create_client, Client

# 1. Supabase ì—°ê²° ì„¤ì •
URL = os.environ.get("SUPABASE_URL")
KEY = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(URL, KEY)

def main():
    print("=== ğŸš¨ ì…˜ íŒ€ì¥ë‹˜ ì „ìš©: ì‹ì•½ì²˜ ë³´ì•ˆ ì™„ì „ ë¬´ë ¥í™” ë° ë°ì´í„° ê°•ì œ ì¸ì¶œ === ")
    
    # [ì„¤ì •] 2ì›” 1ì¼ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€ (íŒ€ì¥ë‹˜ì˜ ì •ë°€ íƒ€ê²© ê¸°ê°„)
    s_start = "2026-02-01"
    s_end = datetime.now().strftime("%Y-%m-%d")
    
    session = requests.Session()
    # í†µí–‰ì¦(Cookie) í™•ë³´ë¥¼ ìœ„í•œ ì²« ë°©ë¬¸
    session.get("https://nedrug.mfds.go.kr/pbp/CCBAE01", timeout=20)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
        'Referer': 'https://nedrug.mfds.go.kr/pbp/CCBAE01',
        'Origin': 'https://nedrug.mfds.go.kr',
        'Content-Type': 'application/x-www-form-urlencoded'
    }

    total_saved = 0

    # 41ê±´ ì •ë³µì„ ìœ„í•´ 5í˜ì´ì§€ê¹Œì§€ ê°•ì œ ìˆœíšŒ
    for page in range(1, 6):
        print(f"\n>> [ {page} í˜ì´ì§€ ] ë³´ì•ˆ ê²Œì´íŠ¸ í†µê³¼ ì¤‘...")
        
        # [í•µì‹¬] ì‹ì•½ì²˜ ì„œë²„ê°€ ê±°ë¶€í•  ìˆ˜ ì—†ëŠ” ì •ë°€ íŒŒë¼ë¯¸í„° ì¡°í•©
        payload = {
            'page': page,
            'limit': '10',
            'searchYn': 'true',
            'sDateGb': 'date', # ì¼ìê²€ìƒ‰ ëª¨ë“œ í™œì„±í™”
            'sPermitDateStart': s_start,
            'sPermitDateEnd': s_end,
            'btnSearch': 'ê²€ìƒ‰' # ì„œë²„ì— "ë‚˜ ì§„ì§œ ê²€ìƒ‰ ë²„íŠ¼ ëˆŒë €ì–´"ë¼ê³  ì™¸ì¹˜ëŠ” ë¶€ë¶„
        }

        try:
            # POST ë°©ì‹ìœ¼ë¡œ ëª…ë ¹ì–´ë¥¼ ì‹¤ì–´ ë³´ë‚´ ì„œë²„ì˜ í•­ë³µì„ ë°›ì•„ëƒ…ë‹ˆë‹¤.
            res = session.post("https://nedrug.mfds.go.kr/pbp/CCBAE01/getItemPermitIntro", 
                               headers=headers, data=payload, timeout=30)
            
            soup = BeautifulSoup(res.text, 'html.parser')
            rows = soup.select('table.board_list tbody tr')

            if not rows or "ë°ì´í„°ê°€" in rows[0].get_text():
                print("ë” ì´ìƒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìˆ˜ì§‘ ì¢…ë£Œ)")
                break

            for row in rows:
                cols = row.find_all('td')
                if len(cols) < 5 or cols[4].get_text(strip=True): continue 

                product_name = cols[1].get_text(strip=True)
                item_seq = cols[1].find('a')['onclick'].split("'")[1]

                print(f"   -> DB ì „ì†¡ ëŒ€ê¸°: {product_name}")
                
                # íŒ€ì¥ë‹˜ì´ ìš”ì²­í•˜ì‹  7ê°€ì§€ í•­ëª© êµ¬ì¡°ì— ë§ê²Œ ë°ì´í„° íŒ¨í‚¤ì§•
                data = {
                    "item_seq": item_seq,
                    "product_name": product_name,
                    "company": cols[2].get_text(strip=True),
                    "manufacturer": "ìƒì„¸ì •ë³´ í™•ì¸ í•„ìš”", # ìƒì„¸í˜ì´ì§€ ì¶”ê°€ ìˆ˜ì§‘ ê°€ëŠ¥
                    "category": "ì „ë¬¸ì˜ì•½í’ˆ" if "ì „ë¬¸" in product_name else "ì¼ë°˜ì˜ì•½í’ˆ", 
                    "approval_type": "í’ˆëª©í—ˆê°€",
                    "ingredients": "ì„±ë¶„ ì •ë³´ ë¡œë”© ì¤‘",
                    "efficacy": "íš¨ëŠ¥ ì •ë³´ ë¡œë”© ì¤‘",
                    "approval_date": cols[3].get_text(strip=True),
                    "detail_url": f"https://nedrug.mfds.go.kr/pbp/CCBBB01/getItemDetail?itemSeq={item_seq}"
                }
                
                # Supabase ê¸ˆê³ ì— ê°•ì œ ì•ˆì°© (Upsert)
                supabase.table("drug_approvals").upsert(data).execute()
                total_saved += 1

            time.sleep(1) # ì„œë²„ì˜ ì˜ì‹¬ì„ í”¼í•˜ê¸° ìœ„í•œ ì§§ì€ íœ´ì‹

        except Exception as e:
            print(f"âš ï¸ {page}í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

    print(f"\n=== ğŸ† ì„±ê³µ: ì´ {total_saved}ê±´ì´ Supabase ê¸ˆê³ ì— ì•ˆì°©í–ˆìŠµë‹ˆë‹¤! ===")

if __name__ == "__main__":
    main()
