import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
from datetime import datetime, timedelta, timezone

# 1. ì„¤ì •
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("ğŸš¨ Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 2. ë‚ ì§œ ì„¤ì • (ğŸ•‘ ì—¬ê¸°ê°€ í•µì‹¬! í•œêµ­ ì‹œê°„ KST ì ìš©)
# GitHub ì„œë²„(UTC)ê°€ ì•„ë‹ˆë¼ 'í•œêµ­ ì‹œê°„' ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œë¥¼ ì¡ì•„ì•¼ 'ì˜¤ëŠ˜' ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
KST = timezone(timedelta(hours=9))
end_date = datetime.now(KST)
start_date = end_date - timedelta(days=14) # ë„‰ë„‰í•˜ê²Œ 2ì£¼ì¹˜ ì¡°íšŒ (ëˆ„ë½ ë°©ì§€)

str_start = start_date.strftime("%Y%m%d")
str_end = end_date.strftime("%Y%m%d")

print(f"=== ğŸ•µï¸â€â™€ï¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (í•œêµ­ì‹œê°„: {str_start} ~ {str_end}) ===")

def run_scraper():
    # 3. URL ìˆ˜ì • (searchType ì œê±° -> ì¡°ê±´ ì—†ì´ ë‚ ì§œë¡œë§Œ ê²€ìƒ‰)
    # pageSize=100 : í•œ ë²ˆì— 100ê°œì”© ê¸ì–´ì˜¤ê¸°
    url = f"https://nedrug.mfds.go.kr/searchDrug/searchDrugList?page=1&searchYn=true&startDate={str_start}&endDate={str_end}&pageSize=100"
    
    # 4. í—¤ë” ì¶”ê°€ (ë´‡ ì°¨ë‹¨ ë°©ì§€ìš© 'ì£¼ë¯¼ë“±ë¡ì¦')
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        
        # í…Œì´ë¸” ì°¾ê¸°
        table = soup.find("div", class_="r_sec").find("table", class_="dr_table")
        if not table:
            print("âŒ ì‹ì•½ì²˜ ì‚¬ì´íŠ¸ì—ì„œ í…Œì´ë¸”ì„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. (êµ¬ì¡° ë³€ê²½ ë˜ëŠ” ì ‘ì† ì°¨ë‹¨)")
            return

        rows = table.find("tbody").find_all("tr")
        print(f"ğŸ” ê²€ìƒ‰ëœ ì˜ì•½í’ˆ ìˆ˜: {len(rows)}ê°œ")

        # 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤' ì²˜ë¦¬
        if len(rows) == 1 and "ê²€ìƒ‰ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" in rows[0].text:
            print(">> í•´ë‹¹ ê¸°ê°„ì— ì‹ ê·œ í—ˆê°€ëœ ì˜ì•½í’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        count = 0
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 2:
                continue
                
            try:
                # ë°ì´í„° ì¶”ì¶œ
                link_tag = cols[1].find("a")
                detail_href = link_tag["href"]
                
                if "itemSeq=" in detail_href:
                    item_seq = detail_href.split("itemSeq=")[1].split("&")[0]
                else:
                    continue

                item_name = link_tag.text.strip()
                company = cols[2].text.strip()
                category = cols[3].text.strip()
                approval_date = cols[4].text.strip()
                
                data = {
                    "item_seq": item_seq,
                    "product_name": item_name,
                    "company": company,
                    "category": category,
                    "approval_date": approval_date,
                    "detail_url": "https://nedrug.mfds.go.kr" + detail_href,
                    # created_at ìƒëµ (DB ìë™ ìƒì„±)
                }

                # Supabase Upsert (ì¤‘ë³µì´ë©´ ë¬´ì‹œ/ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€)
                result = supabase.table("drug_approvals").upsert(data, on_conflict="item_seq").execute()
                count += 1
                
            except Exception as e:
                print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ ({item_name}): {e}")
                continue

        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {count}ê±´ ì²˜ë¦¬ë¨")

    except Exception as e:
        print(f"ğŸš¨ ìŠ¤í¬ë˜í•‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    run_scraper()
